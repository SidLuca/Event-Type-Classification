# -*- coding: utf-8 -*-
"""Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GDoYZOWYhkyWFIw3xLHqwv22fpuhdWi1
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score,
    roc_curve, precision_recall_curve, confusion_matrix
)
import warnings, joblib
warnings.filterwarnings('ignore')
sns.set(style="whitegrid")

print("ðŸ“¥ Loading dataset...")
df = pd.read_csv('events.csv')

if 'type' not in df.columns:
    raise ValueError("Dataset must contain a 'type' column as target.")

# Binary target: BBar (0/1)
df['isBBar'] = df['type'].apply(lambda x: 1 if x in [0, 1] else 0)

# Remove NaNs and infinities
df.replace([np.inf, -np.inf], np.nan, inplace=True)
df.dropna(inplace=True)
print(f"âœ… Clean data: {df.shape[0]} rows, {df.shape[1]} columns")

features = [c for c in df.columns if c not in ['type', 'isBBar']]
X = df[features]
y = df['isBBar']

rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
rf.fit(X, y)
importances = pd.DataFrame({'Feature': features, 'Importance': rf.feature_importances_})
importances = importances.sort_values('Importance', ascending=False).head(25)
selected_features = importances['Feature'].tolist()
print(f"ðŸ“Š Using top {len(selected_features)} features.")
importances

X_train, X_test, y_train, y_test = train_test_split(
    df[selected_features], y, test_size=0.2, random_state=42, stratify=y
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"Training set: {X_train.shape[0]} rows, Test set: {X_test.shape[0]} rows")

try:
    from xgboost import XGBClassifier
    from lightgbm import LGBMClassifier
except ImportError:
    print("Install with: pip install xgboost lightgbm")
    raise

models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42, n_jobs=-1),
    "XGBoost": XGBClassifier(
        n_estimators=300, learning_rate=0.05, max_depth=6,
        scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1]),
        subsample=0.8, colsample_bytree=0.8, random_state=42, eval_metric='logloss'
    ),
    "LightGBM": LGBMClassifier(
        n_estimators=300, learning_rate=0.05, num_leaves=31,
        class_weight='balanced', random_state=42, n_jobs=-1
    ),
    # Added Models
    "Neural Network (MLP)": MLPClassifier(
        hidden_layer_sizes=(64, 32, 16),
        activation='relu',
        solver='adam',
        alpha=0.001,
        batch_size=64,
        max_iter=500,
        random_state=42
    ),
    "SVM": SVC(
        kernel='rbf',
        probability=True,
        class_weight='balanced',
        random_state=42
    )
}

results = []
probabilities = {}

for name, model in models.items():
    print(f"\nðŸš€ Training {name}...")
    use_scaled = name in ["Logistic Regression", "Neural Network (MLP)", "SVM"]
    Xtr, Xte = (X_train_scaled, X_test_scaled) if use_scaled else (X_train, X_test)
    model.fit(Xtr, y_train)
    y_pred = model.predict(Xte)
    y_proba = model.predict_proba(Xte)[:, 1] if hasattr(model, "predict_proba") else None

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_proba) if y_proba is not None else np.nan
    pr_auc = average_precision_score(y_test, y_proba) if y_proba is not None else np.nan

    results.append({
        "Model": name, "Accuracy": acc, "Precision": prec, "Recall": rec,
        "F1": f1, "ROC-AUC": roc_auc, "PR-AUC": pr_auc
    })

    probabilities[name] = (y_pred, y_proba)

results_df = pd.DataFrame(results)
pd.set_option('display.max_columns', None)
results_df

plt.figure(figsize=(10, 6))
bar_width = 0.13
x = np.arange(len(results_df["Model"]))
metrics = ["Accuracy", "Precision", "Recall", "F1", "ROC-AUC", "PR-AUC"]

for i, metric in enumerate(metrics):
    plt.bar(x + i * bar_width, results_df[metric], width=bar_width, label=metric)

plt.xticks(x + bar_width * (len(metrics) - 1) / 2, results_df["Model"], rotation=25)
plt.ylabel("Score")
plt.title("Model Performance Comparison")
plt.legend()
plt.tight_layout()
plt.show()

rows = int(np.ceil(len(models) / 2))
fig, axes = plt.subplots(rows, 3, figsize=(12, rows * 4))
axes = axes.ravel()

for i, (name, (y_pred, _)) in enumerate(probabilities.items()):
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])
    axes[i].set_title(f"{name}")
    axes[i].set_xlabel("Predicted")
    axes[i].set_ylabel("True")

for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 6))
for name, (_, y_proba) in probabilities.items():
    if y_proba is not None:
        fpr, tpr, _ = roc_curve(y_test, y_proba)
        auc = roc_auc_score(y_test, y_proba)
        plt.plot(fpr, tpr, label=f"{name} (AUC={auc:.3f})")

plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curves")
plt.legend()
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 6))
for name, (_, y_proba) in probabilities.items():
    if y_proba is not None:
        precision, recall, _ = precision_recall_curve(y_test, y_proba)
        auc = average_precision_score(y_test, y_proba)
        plt.plot(recall, precision, label=f"{name} (AUC={auc:.3f})")

plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precisionâ€“Recall Curves")
plt.legend()
plt.tight_layout()
plt.show()

best_model_name = results_df.sort_values("ROC-AUC", ascending=False).iloc[0]["Model"]
best_model = models[best_model_name]
joblib.dump(best_model, f"best_model_{best_model_name.replace(' ', '_').lower()}.pkl")
print(f"\nâœ… Best model: {best_model_name} â€” saved to disk.")